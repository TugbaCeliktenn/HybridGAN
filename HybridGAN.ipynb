{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OyP2GAz_x2z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "import gc\n",
        "import csv\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from numpy import dstack\n",
        "use_gpu=True\n",
        "import keras\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "from keras.layers import Lambda\n",
        "from keras import layers\n",
        "from operator import add\n",
        "from ast import literal_eval\n",
        "from matplotlib import pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "from collections import Counter\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import InputSpec, Layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from keras.layers.core import Lambda\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model, Sequential, load_model, model_from_json\n",
        "from keras.layers import Dense, Dropout, Activation, SpatialDropout1D, GaussianNoise, GaussianDropout\n",
        "from keras.layers import Input, Embedding, Flatten, Conv1D, Conv2D, Reshape, Concatenate, concatenate\n",
        "from keras.layers import LeakyReLU, LSTM, GRU, Bidirectional, BatchNormalization, TimeDistributed, Add\n",
        "from keras.layers import GlobalMaxPooling1D, MaxPooling1D, MaxPool2D, GlobalAveragePooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, Callback, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import resample\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, auc, f1_score, classification_report, accuracy_score, confusion_matrix\n",
        "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score, roc_curve, auc\n",
        "from keras.layers import Layer\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6axmiQcFAUnY",
        "outputId": "6e6f8b16-13ac-4a46-a758-a72e9e470083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toplam tekrarlanan satır sayısı: 0\n"
          ]
        }
      ],
      "source": [
        "path = '...'\n",
        "FREQ_DIST_FILE = path+'/processed_freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = path+'/processed_freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE =  path+'/processed_train.csv'\n",
        "TEST_PROCESSED_FILE = path+'/processed_test.csv'\n",
        "processed_data_csv = path+'/processed_dataset.csv'\n",
        "cols = ['abstract','category']\n",
        "df = pd.read_csv(processed_data_csv, header = None, names = cols, encoding = 'utf-8')\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True,inplace=True)\n",
        "x = df.abstract\n",
        "y = df.category\n",
        "num_classes = 3\n",
        "SEED = 200\n",
        "max_length = 150\n",
        "df.drop_duplicates(inplace=True)\n",
        "duplicate_rows = df[df.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbouHU2IB8KI"
      },
      "outputs": [],
      "source": [
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zax5sbi8CMBK"
      },
      "outputs": [],
      "source": [
        "def process_abstracts(csv_file, test_file):\n",
        "    abstracts = []\n",
        "    labels = []\n",
        "    with open(csv_file, 'r', encoding = \"utf-8\") as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        category_names = []\n",
        "        for i, line in enumerate(lines):\n",
        "            abstract = line[:line.find(',')].strip()\n",
        "            category = line[1 + line.find(','):].strip().strip('\\\"').strip('\\n').strip('\\;')\n",
        "            abstracts.append(abstract)\n",
        "            labels.append(category)\n",
        "            if category not in category_names:\n",
        "                category_names.append(category)\n",
        "    return abstracts, labels, category_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqhbhc6JVEhl"
      },
      "outputs": [],
      "source": [
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_q = self.add_weight(name=\"W_q\", shape=(input_shape[-1], input_shape[-1]), initializer=\"glorot_uniform\", trainable=True)\n",
        "        self.W_k = self.add_weight(name=\"W_k\", shape=(input_shape[-1], input_shape[-1]), initializer=\"glorot_uniform\", trainable=True)\n",
        "        self.W_v = self.add_weight(name=\"W_v\", shape=(input_shape[-1], input_shape[-1]), initializer=\"glorot_uniform\", trainable=True)\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        q = tf.matmul(x, self.W_q)\n",
        "        k = tf.matmul(x, self.W_k)\n",
        "        v = tf.matmul(x, self.W_v)\n",
        "\n",
        "        attn_scores = tf.matmul(q, k, transpose_b=True)\n",
        "        attn_scores = tf.nn.softmax(attn_scores)\n",
        "\n",
        "        output = tf.matmul(attn_scores, v)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6qvPKn8CqsB"
      },
      "outputs": [],
      "source": [
        "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.2, random_state=SEED)\n",
        "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "isb-1-LNCMS1"
      },
      "outputs": [],
      "source": [
        "### tokenize data using bert tokenizer\n",
        "from transformers import *\n",
        "#Get BertTokenizer\n",
        "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "vocab_size = len(tokenizer.get_vocab())\n",
        "model = BertModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPVgQr5d7AT8"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = model.embeddings.word_embeddings.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zaI0v-lLe3y"
      },
      "outputs": [],
      "source": [
        "numpy_embedding_matrix = embedding_matrix.detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWqeOz_K7f0z"
      },
      "outputs": [],
      "source": [
        "embedding_dim = model.embeddings.word_embeddings.weight.size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJJRhh0jCeCr"
      },
      "outputs": [],
      "source": [
        "tokenized_x_train = [tokenizer.tokenize(com) for com in x_train]\n",
        "tokenized_x_train = [sent[:max_length] for sent in tokenized_x_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obwlOCvpDg_H"
      },
      "outputs": [],
      "source": [
        "tokenized_x_validation = [tokenizer.tokenize(com) for com in x_validation]\n",
        "tokenized_x_validation = [sent[:max_length] for sent in tokenized_x_validation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GawMfu-DhBf"
      },
      "outputs": [],
      "source": [
        "tokenized_x_test = [tokenizer.tokenize(com) for com in x_test]\n",
        "tokenized_x_test = [sent[:max_length] for sent in tokenized_x_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HanjdqGWCzHo"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenized_x_train)):\n",
        "    sent = tokenized_x_train[i]\n",
        "    sent = ['[CLS]'] + sent + ['[SEP]']\n",
        "    tokenized_x_train[i] = sent\n",
        "#Convert tokens into IDs\n",
        "input_ids_x_train = [tokenizer.convert_tokens_to_ids(com) for com in tokenized_x_train]\n",
        "input_ids_x_train = tf.keras.preprocessing.sequence.pad_sequences(input_ids_x_train, maxlen=max_length+2, truncating='post', padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0UPG1_sD7Fr"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenized_x_validation)):\n",
        "    sent = tokenized_x_validation[i]\n",
        "    sent = ['[CLS]'] + sent + ['[SEP]']\n",
        "    tokenized_x_validation[i] = sent\n",
        "input_ids_x_validation = [tokenizer.convert_tokens_to_ids(com) for com in tokenized_x_validation]\n",
        "#Pad our tokens which might be less than max_length size\n",
        "input_ids_x_validation = tf.keras.preprocessing.sequence.pad_sequences(input_ids_x_validation, maxlen=max_length+2, truncating='post', padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-ODAacuD7Io"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenized_x_test)):\n",
        "    sent = tokenized_x_test[i]\n",
        "    sent = ['[CLS]'] + sent + ['[SEP]']\n",
        "    tokenized_x_test[i] = sent\n",
        "input_id_x_test = [tokenizer.convert_tokens_to_ids(com) for com in tokenized_x_test]\n",
        "#Pad our tokens which might be less than max_length size\n",
        "input_ids_x_test = tf.keras.preprocessing.sequence.pad_sequences(input_id_x_test, maxlen=max_length+2, truncating='post', padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oN4vsFIEhTx"
      },
      "outputs": [],
      "source": [
        "y_train              = np.array(y_train)\n",
        "y_validation         = np.array(y_validation)\n",
        "y_test               = np.array(y_test)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train              = label_encoder.fit_transform(y_train)\n",
        "y_validation         = label_encoder.transform(y_validation)\n",
        "y_test               = label_encoder.transform(y_test)\n",
        "#-------------------------\n",
        "y_train              = to_categorical(y_train, num_classes=num_classes)\n",
        "y_validation         = to_categorical(y_validation, num_classes=num_classes)\n",
        "y_test               = to_categorical(y_test, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cXu8dN5kP5Tk",
        "outputId": "a8264377-c52b-44ce-ff73-ecb0043edfa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "#In[ ]---------------------MODEL------------------------------------\n",
        "file_path = pathModels + \"HybridGAN.hdf5\"\n",
        "check_point = ModelCheckpoint(file_path, monitor=\"val_accuracy\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "early_stop = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5)\n",
        "m_name = 'HybridGAN_'\n",
        "inp = Input(shape=(max_length+2,), name=m_name+'inp')\n",
        "x_gNoise=GaussianNoise(0.2)(inp)\n",
        "#embd = Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length+2)(x_gNoise)\n",
        "embd = Embedding(numpy_embedding_matrix.shape[0], embedding_dim, weights = [numpy_embedding_matrix], trainable = True)(x_gNoise)\n",
        "\n",
        "x_LSTM= LSTM(64, return_sequences=True)(embd)\n",
        "x_sDrp = SpatialDropout1D(0.2, name = 'spdrop_1')(x_LSTM)\n",
        "\n",
        "x_biLSTM = Bidirectional(LSTM(units=64, return_sequences=True))(x_sDrp)\n",
        "x_biLSTM_reshaped = Reshape((-1, 2*64))(x_biLSTM)\n",
        "x_biLSTM_att = AttentionWithContext()(x_biLSTM_reshaped)\n",
        "x_dropout1 = Dropout(0.2, name=m_name+'dropout_1')(x_biLSTM_att)\n",
        "\n",
        "x_biGRU = Bidirectional(GRU(units=64, return_sequences=True))(x_dropout1)\n",
        "x_dropout2 = Dropout(0.2, name=m_name+'dropout_2')(x_biGRU)\n",
        "x_pool2 = GlobalMaxPooling1D(name=m_name+'pool_1')(x_dropout2)\n",
        "\n",
        "dense1 = Dense(64, activation='softmax', name=m_name+'dense')(x_pool2)\n",
        "\n",
        "output = Dense(num_classes,  name=m_name+'out')(dense1)\n",
        "\n",
        "model = Model(inputs=inp, outputs=output)\n",
        "optimizer = Adam(learning_rate=1e-3)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "#print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5K9Pm2FH4ISm",
        "outputId": "d7d47d01-ecc2-4b2c-9fa1-09bdb1537638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "335/335 [==============================] - ETA: 0s - loss: 6.5257 - accuracy: 0.1135\n",
            "Epoch 1: val_accuracy improved from -inf to 0.33920, saving model to /content/drive/MyDrive/doktora/PROJECT/RADIOLOGY/BertTokenDL/HybridGenAIDetection.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r335/335 [==============================] - 294s 857ms/step - loss: 6.5257 - accuracy: 0.1135 - val_loss: 10.6177 - val_accuracy: 0.3392\n",
            "Epoch 2/6\n",
            "335/335 [==============================] - ETA: 0s - loss: 10.6780 - accuracy: 0.3356\n",
            "Epoch 2: val_accuracy did not improve from 0.33920\n",
            "335/335 [==============================] - 273s 815ms/step - loss: 10.6780 - accuracy: 0.3356 - val_loss: 10.6177 - val_accuracy: 0.3392\n",
            "Epoch 3/6\n",
            "335/335 [==============================] - ETA: 0s - loss: 10.6727 - accuracy: 0.3354\n",
            "Epoch 3: val_accuracy did not improve from 0.33920\n",
            "335/335 [==============================] - 270s 807ms/step - loss: 10.6727 - accuracy: 0.3354 - val_loss: 10.6177 - val_accuracy: 0.3392\n",
            "Epoch 4/6\n",
            "335/335 [==============================] - ETA: 0s - loss: 10.6775 - accuracy: 0.3356\n",
            "Epoch 4: val_accuracy did not improve from 0.33920\n",
            "335/335 [==============================] - 268s 800ms/step - loss: 10.6775 - accuracy: 0.3356 - val_loss: 10.6177 - val_accuracy: 0.3392\n",
            "Epoch 5/6\n",
            "335/335 [==============================] - ETA: 0s - loss: 10.6798 - accuracy: 0.3356\n",
            "Epoch 5: val_accuracy did not improve from 0.33920\n",
            "335/335 [==============================] - 266s 795ms/step - loss: 10.6798 - accuracy: 0.3356 - val_loss: 10.6177 - val_accuracy: 0.3392\n",
            "Epoch 6/6\n",
            "335/335 [==============================] - ETA: 0s - loss: 10.6847 - accuracy: 0.3356\n",
            "Epoch 6: val_accuracy did not improve from 0.33920\n",
            "335/335 [==============================] - 265s 792ms/step - loss: 10.6847 - accuracy: 0.3356 - val_loss: 10.6177 - val_accuracy: 0.3392\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(input_ids_x_train, y_train, batch_size=128, epochs=6, validation_data=(input_ids_x_validation, y_validation),\n",
        "                    verbose=1, callbacks=[check_point, early_stop])\n",
        "#In[ ]---------------------/MODEL------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8aByqtK4h4v_"
      },
      "outputs": [],
      "source": [
        "# Tüm eğitim ve doğrulama loss değerlerini alın\n",
        "all_train_loss = history.history['loss']\n",
        "all_val_loss = history.history['val_loss']\n",
        "\n",
        "# Tüm epoch'ların perplexity değerlerini hesaplayın\n",
        "all_train_perplexity = [2 ** loss for loss in all_train_loss]\n",
        "all_val_perplexity = [2 ** loss for loss in all_val_loss]\n",
        "\n",
        "# Ortalama perplexity değerlerini hesaplayın\n",
        "avg_train_perplexity = sum(all_train_perplexity) / len(all_train_perplexity)\n",
        "avg_val_perplexity = sum(all_val_perplexity) / len(all_val_perplexity)\n",
        "\n",
        "# Elde ettiğiniz ortalama perplexity değerlerini kullanabilirsiniz\n",
        "print(\"Ortalama Eğitim Perplexity -HybridGAN:\", avg_train_perplexity)\n",
        "print(\"Ortalama Doğrulama Perplexity -HybridGAN:\", avg_val_perplexity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WetknazICaOa"
      },
      "outputs": [],
      "source": [
        "# Modeli test veri seti üzerinde değerlendirin\n",
        "test_loss = model.evaluate(input_ids_x_test, y_test, verbose=0)[0]\n",
        "# Test veri seti üzerindeki perplexity değerini hesaplayın\n",
        "test_perplexity = 2 ** test_loss\n",
        "# Elde ettiğiniz test perplexity değerini kullanabilirsiniz\n",
        "print(\"Test Perplexity:\", test_perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICnM8vAqQk1u"
      },
      "outputs": [],
      "source": [
        "loaded_mymodel= load_model(pathModels + 'HybridGAN.hdf5', custom_objects={'AttentionWithContext': AttentionWithContext , \"tf\": tf})\n",
        "print('loaded_mymodel loaded')\n",
        "yhat_mymodel = loaded_mymodel.predict(input_ids_x_test)\n",
        "print('yhat_mymodel complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9TTf_-2TF5x"
      },
      "outputs": [],
      "source": [
        "classifiers = [yhat_mymodel]\n",
        "for c,i in zip(classifiers,range(len(classifiers))):\n",
        "  y_pred = (c > 0.5).astype(np.int32)\n",
        "  y_pred = y_pred.tolist()\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "multilabel_conf_matrix = multilabel_confusion_matrix(y_pred, y_test)\n",
        "print(multilabel_conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxkInTK4Tucp"
      },
      "outputs": [],
      "source": [
        "category_names = ['Human', 'Paraphrased', 'GPT']\n",
        "#----------------\n",
        "print(\"HybridGAN\")\n",
        "for class_index, conf_matrix in enumerate(multilabel_conf_matrix):\n",
        "    class_name = category_names[class_index]\n",
        "    print(f\"Confusion Matrix for Class {class_name}:\\n\\n\", conf_matrix)\n",
        "    TP, FP, FN, TN = conf_matrix.ravel()\n",
        "    print(f'TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN} \\n')\n",
        "    y_test_class = y_test[:, class_index]\n",
        "    y_pred_class = y_pred[:, class_index]\n",
        "    #conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
        "    acc = accuracy_score(y_test_class, y_pred_class)\n",
        "    f1 = f1_score(y_test_class, y_pred_class)\n",
        "    precision = precision_score(y_test_class, y_pred_class)\n",
        "    recall = recall_score(y_test_class, y_pred_class)\n",
        "    mcc = matthews_corrcoef(y_test_class, y_pred_class)\n",
        "    kappa = cohen_kappa_score(y_test_class, y_pred_class)\n",
        "    roc_curve = roc_auc_score(y_test_class, y_pred_class)\n",
        "    accuracy_values = []\n",
        "    metrics = []\n",
        "#--------------------------------------------\n",
        "    accuracy_values.append(round(acc, 2))\n",
        "#--------------------------------------------\n",
        "    metrics.append({\n",
        "        \"Class\": class_name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1 Score\": f1,\n",
        "        \"roc_curve\":roc_curve,\n",
        "        \"mcc\":mcc,\n",
        "        \"kappa\":kappa\n",
        "    })\n",
        "    print(f'acc - {round(acc, 2)}')\n",
        "    print(f'f1 - {round(f1, 2)}')\n",
        "    print(f'precision - {round(precision, 2)}')\n",
        "    print(f'recall - { round(recall, 2)}')\n",
        "    print(f'mcc - {round(mcc, 2)}')\n",
        "    print(f'kappa - {round(kappa, 2)}')\n",
        "    print(f'roc_curve - {round(roc_curve, 2)} \\n')\n",
        "#--------------------------------------------\n",
        "    X_ = [\"Accuracy\",\"F1 Score\", \"Precision\", \"Recall\", \"MCC\", \"Kappa\", \"Roc Curve\"]\n",
        "    v_ = accuracy_values\n",
        "\n",
        "    colors = [cm.inferno(i / 7) for i in range(7)]\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    bars = plt.bar(X_, v_, color=colors, width=0.5)\n",
        "    plt.bar(X_, v_, color=colors, width=0.5)\n",
        "    plt.ylabel('Values')\n",
        "    plt.title(class_name+\"\\n\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 1)  # Y ekseni başlangıcı ve bitişi\n",
        "    plt.yticks(np.arange(0, 1.1, 0.25))  # Y ekseni işaretleme aralığı\n",
        "\n",
        "    # Barların üzerine değerleri ekleme\n",
        "    for bar, value in zip(bars, v_):\n",
        "      plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, value + 0.02, round(value, 2), ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "    print('\\n')\n",
        "\n",
        "# Tüm sınıfların ağırlıklı ortalamasını hesaplayın\n",
        "overall_accuracy = np.average(accuracy_values)\n",
        "\n",
        "# Genel performans metriklerini ekrana yazdırın\n",
        "print(f\"Genel Doğruluk (Overall Accuracy): {round(overall_accuracy, 2)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}